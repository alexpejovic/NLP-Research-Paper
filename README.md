# NLP-Research-Paper

A research paper and experiment discovering the importance of the specificity of pre-training datasets on large language model architecture BERT.
Did this by comparing models RoBERTa and BERTweet on Twitter specific NLP tasks. 

RoBERTa and BERTweet are pre-trained models of the exact same architecture, differing only in pre-training datasets. RoBERTa was trained on 160GB of
general texts, while BERTweet was trained on 80GB of Tweets. Comparing the performance of these two models on Twitter-specific tasks yielded interesting
results on the efficacy of domain-specific pre-training for large language models.
